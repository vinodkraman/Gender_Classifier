# -*- coding: utf-8 -*-
"""Transfer_Learning/Gender_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kkXN3EMy3tU1WnXf2HEQlqdFNzXwk7IL

# Transfer Learning and Gender Classification

Please update all file_paths where indicated by comments.
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

"""# Data Preprocessing"""

#Download tar file
!wget https://s3.amazonaws.com/matroid-web/datasets/agegender_cleaned.tar.gz

#unzip
!tar --gunzip --extract --verbose --file=agegender_cleaned.tar.gz

import PIL
import pathlib

#view some images
src_path = '/content/combined/aligned/' #UPDATE SRC_PATH TO YOUR CORRECT DIRECTORY
data_dir = pathlib.Path(src_path)
females = list(data_dir.glob('25_F/*'))
PIL.Image.open(str(females[10]))

#samples num_samples number of images from each folder in the dataset directory and appends it to file_paths and labels
def subsample_from_folder(data_dir, num_samples, file_paths, labels):
  for i in range(70):
    files_in_fold_M = [str(file) for file in list(data_dir.glob(str(i+1).zfill(2) + '_M/*'))]
    files_in_fold_F = [str(file) for file in list(data_dir.glob(str(i+1).zfill(2) + '_F/*'))]

    size_M = len(list(data_dir.glob(str(i+1).zfill(2) + '_M/*')))
    random_labels_M = random.sample(range(size_M), min(num_samples, size_M))
    for label in random_labels_M:
      file_paths.append(files_in_fold_M[label])
      labels.append(0)

    size_F = len(list(data_dir.glob(str(i+1).zfill(2) + '_F/*')))
    random_labels_F = random.sample(range(size_F), min(num_samples, size_F))
    for label in random_labels_F:
      file_paths.append(files_in_fold_F[label])
      labels.append(1)

#construct a training list along with label by sampling from dataset
import cv2
import numpy as np
np.random.seed(0)
import random
random.seed(0) 

file_paths = []
labels = []
num_samples = 20

#aligned
aligned_path = '/content/combined/aligned' #UPDATE ALIGNED_PATH TO YOUR CORRECT DIRECTORY
data_dir = pathlib.Path(aligned_path)
subsample_from_folder(data_dir, num_samples, file_paths, labels)

print(len(file_paths))

#combine file paths and labels in order to split in train and test sets
file_paths = np.array(file_paths).reshape(len(file_paths), 1)
labels = np.array(labels).reshape(len(file_paths), 1)
merged_data = np.concatenate((file_paths, labels), axis=1)
print(merged_data.shape)
print("pre shuffle", merged_data)

#shuffle
np.random.shuffle(merged_data)
print("post shuffle", merged_data)

#index
end_train = int(len(file_paths)*0.60)
end_vald = int(len(file_paths)*0.60) + int(len(file_paths)*0.20)

#extract train, validation, and test sets
train = merged_data[:end_train,:]
valid = merged_data[end_train:end_vald,:]
test = merged_data[end_vald:,:]

print(len(train))
print(len(valid))
print((len(test)))

#replace file paths with numpy images
images_train = []
labels_train = []

images_valid = []
labels_valid = []

images_test = []
labels_test = []

WIDTH, HEIGHT = 224, 224

for row in train:
  im = cv2.imread(row[0])

  im = cv2.resize(im,  (WIDTH, HEIGHT))
  images_train.append(im)
  labels_train.append(float(row[1]))

for row in valid:
  im = cv2.imread(row[0])

  im = cv2.resize(im,  (WIDTH, HEIGHT))
  images_valid.append(im)
  labels_valid.append(float(row[1]))

for row in test:
  im = cv2.imread(row[0])

  im = cv2.resize(im,  (WIDTH, HEIGHT))
  images_test.append(im)
  labels_test.append(float(row[1]))

print(np.array(images_train).shape)

#compute mean training image for normalization
mean_img = np.mean(images_train, axis=0)
print(mean_img.shape)

norm_images_train = np.array(images_train) - mean_img
norm_images_valid = np.array(images_valid) - mean_img
norm_images_test = np.array(images_test) - mean_img

print(norm_images_train.shape)
print(norm_images_valid.shape)
print(norm_images_test.shape)

#port training, validation, and test datasets into tf.Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((norm_images_train, labels_train))
valid_dataset = tf.data.Dataset.from_tensor_slices((norm_images_valid, labels_valid))

BATCH_SIZE = 20
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
valid_dataset = valid_dataset.batch(BATCH_SIZE)

for elem in train_dataset:
  print(elem)

#pre-fetch batches to speed up
AUTOTUNE = tf.data.experimental.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
valid_dataset = valid_dataset.prefetch(buffer_size=AUTOTUNE)

"""# Model Building"""

#Reconstruct VGG Face Descriptor
base_model = models.Sequential()
base_model.add(layers.Conv2D(64, (3, 3), padding= "same", activation='relu', input_shape=(224, 224, 3)))
base_model.add(layers.Conv2D(64, (3, 3),padding= "same", activation='relu'))
base_model.add(layers.MaxPooling2D(2, 2))
base_model.add(layers.Conv2D(128, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(128, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.MaxPooling2D(2, 2))
base_model.add(layers.Conv2D(256, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(256, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(256, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.MaxPooling2D(2, 2))
base_model.add(layers.Conv2D(512, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(512, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(512, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.MaxPooling2D(2, 2))
base_model.add(layers.Conv2D(512, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(512, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.Conv2D(512, (3, 3), padding= "same", activation='relu'))
base_model.add(layers.MaxPooling2D(2, 2))
base_model.summary()

#Load weights - #UPDATE SRC_PATH TO YOUR CORRECT DIRECTORY
src_path = "/content/drive/My Drive/weights_truncated.h5"
base_model.load_weights(src_path)
weights, biases = base_model.layers[0].get_weights()
print(weights)
base_model.summary()

#freeze weights
base_model.trainable = False
base_model.summary()

#Add classification head
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
fc_1 = tf.keras.layers.Dense(32, activation="relu")
prediction_layer = tf.keras.layers.Dense(1)

inputs = tf.keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = global_average_layer(x)
x = fc_1(x)
x = tf.keras.layers.Dropout(0.25)(x)
outputs = prediction_layer(x)
new_model = tf.keras.Model(inputs, outputs)

new_model.summary()

"""# Training"""

#compile new model
base_learning_rate = 0.001
new_model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

new_model.summary()

#train new model for 10 epochs
epochs=10
history = new_model.fit(
  train_dataset,
  validation_data=valid_dataset,
  epochs=epochs
)

#save model weights and architecture
new_model.save("weights_arch_transfer_model")

#save model weights and architecture
new_model.save("weights_arch_transfer_model_h5.h5")

#plot learning curves - note because I used dropout, the network performs differently on training and validation data
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""# Evaluation

Overall Metrics
"""

# Overall
test_dataset_overall = tf.data.Dataset.from_tensor_slices((norm_images_test, labels_test))
test_dataset_overall = test_dataset_overall.batch(20)

#make predictions on entire test set images
predictions = new_model.predict(test_dataset_overall, verbose=1).flatten()
#apply sigmoid since net returns logit
predictions = tf.nn.sigmoid(predictions)
predictions = tf.where(predictions < 0.5, 0, 1)

#compute metric
m = tf.keras.metrics.Accuracy()
m.update_state(labels_test, predictions)
acc_overall = m.result().numpy()
print("Overall Test Accuracy: ", acc_overall)

m = tf.keras.metrics.Recall()
m.update_state(labels_test, predictions)
recall_overall = m.result().numpy()
print("Overall Test Recall: ", recall_overall)

m = tf.keras.metrics.Precision()
m.update_state(labels_test, predictions)
prec_overall = m.result().numpy()
print("Overall Test Precision: ", prec_overall)

m = tf.keras.metrics.AUC()
m.update_state(labels_test, predictions)
auc_overall = m.result().numpy()
print("Overall Test AUC: ", auc_overall)

TN = np.count_nonzero((np.array(predictions - 1)) * (np.array(labels_test) - 1))
FP = np.count_nonzero(np.array(predictions) * (np.array(labels_test) - 1))
print("Overall Test Specificity: ", TN/(TN+FP))

"""Male Metrics"""

#extract only male images from testing set
normalized_test_images_male = np.array([img for (index, img) in enumerate(norm_images_test) if labels_test[index] == 0])
print(normalized_test_images_male.shape)
labels_test_male = np.array([label for label in labels_test if label == 0])
print(labels_test_male.shape)
test_dataset_male = tf.data.Dataset.from_tensor_slices((normalized_test_images_male, labels_test_male))
test_dataset_male = test_dataset_male.batch(20)

#make predictions on male images of test set
predictions_male = new_model.predict(test_dataset_male, verbose=1).flatten()
#apply sigmoid since net returns logit
predictions_male = tf.nn.sigmoid(predictions_male)
predictions_male = tf.where(predictions_male < 0.5, 0, 1)

#compute metrics
m = tf.keras.metrics.Accuracy()
m.update_state(labels_test_male, predictions_male)
acc_male = m.result().numpy()
print("Male Test Accuracy: ", acc_male)

m = tf.keras.metrics.Recall()
m.update_state(labels_test_male, predictions_male)
recall_male = m.result().numpy()
print("Male Test Recall: ", recall_male)

m = tf.keras.metrics.Precision()
m.update_state(labels_test_male, predictions_male)
prec_male = m.result().numpy()
print("Male Test Precision: ", prec_male)

m = tf.keras.metrics.AUC()
m.update_state(labels_test_male, predictions_male)
auc_male = m.result().numpy()
print("Male Test AUC: ", auc_male)

TN = np.count_nonzero((np.array(predictions_male - 1)) * (np.array(labels_test_male) - 1))
FP = np.count_nonzero(np.array(predictions_male) * (np.array(labels_test_male) - 1))
print("Male Test Specificity: ", TN/(TN+FP))

"""Female Metrics"""

#extract only female images from testing set
normalized_test_images_female = np.array([img for (index, img) in enumerate(norm_images_test) if labels_test[index] == 1])
print(normalized_test_images_female.shape)
labels_test_female = np.array([label for label in labels_test if label == 1])
print(labels_test_female.shape)
test_dataset_female = tf.data.Dataset.from_tensor_slices((normalized_test_images_female, labels_test_female))
test_dataset_female = test_dataset_female.batch(20)

#make predictions on male images of test set
predictions_female = new_model.predict(test_dataset_female, verbose=1).flatten()
#apply sigmoid since net returns logit
predictions_female = tf.nn.sigmoid(predictions_female)
predictions_female = tf.where(predictions_female < 0.5, 0, 1)

#compute metrics
m = tf.keras.metrics.Accuracy()
m.update_state(labels_test_female, predictions_female)
acc_female = m.result().numpy()
print("Female Test Accuracy: ", acc_female)

m = tf.keras.metrics.Recall()
m.update_state(labels_test_female, predictions_female)
recall_female = m.result().numpy()
print("Female Test Recall: ", recall_female)

m = tf.keras.metrics.Precision()
m.update_state(labels_test_female, predictions_female)
prec_female = m.result().numpy()
print("Female Test Precision: ", prec_female)

m = tf.keras.metrics.AUC()
m.update_state(labels_test_female, predictions_female)
auc_female = m.result().numpy()
print("Female Test AUC: ", auc_female)
